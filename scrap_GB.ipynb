{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping top repositories on GitHub\n",
    "\n",
    "INTRO - \n",
    "- Web scraping is the process of extracting and parsing data from websites in an automated fashion using a computer program. It's a useful technique for creating datasets for research and learning.\n",
    "- GitHub:\n",
    " GitHub is a hosting site where developers and programmers can upload the code they create and work collaboratively to improve it. An important feature of GitHub is its version control system. The version control lets coders tweak software—potentially fixing bugs or improving efficiency—without affecting the software itself or risking the experience of any current users.\n",
    "- In this project we are using the scraping technique to scrap GitHub for the top 30 repositories.\n",
    "- Tools Used -  Python,Beautiful soup,Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps we'll follow:\n",
    "\n",
    "- We're going to scrape https://github.com/topics.\n",
    "- We'll get a list of topics. For each topic, we'll get topic title, topic page URL and topic description.\n",
    "- For each topic, we'll get the top 30 repositories in the topic from the topic page.\n",
    "- For each repository, we'll grab the repo name, username, stars and repo URL.\n",
    "- For each topic we'll create a CSV file in the following format:\n",
    "    Repo Name,Username,Stars,Repo URL\n",
    "    - Example:\n",
    "\n",
    "      three.js,mrdoob,180700,https://github.com/mrdoob/three.js\n",
    "      \n",
    "      libgdx,libgdx,20300,https://github.com/libgdx/libgdx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the list of topics from GitHub\n",
    "- Use requests to download the page\n",
    "- Use Beautifulsoup to parse it the HTML format and extract some information\n",
    "- Next we will convert it to a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_topics_page():\n",
    "    # Downloading the required page\n",
    "    topics_url = 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    # parsing using BeautifulSoup\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are creating some helper functions to parse information from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    topic_title_tags = doc.find_all('p',{\"class\":\"f3 lh-condensed mb-0 mt-1 Link--primary\"})\n",
    "    topic_titles =[]\n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_topic_titles can be used to get the list of titles\n",
    "#### Similarly we define functions for descriptions and url of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_descs(doc):\n",
    "    topic_desc_tags= doc.find_all(\"p\",{\"class\":\"f5 color-fg-muted mb-0 mt-1\"})\n",
    "    topic_descs=[]\n",
    "    for tag in topic_desc_tags:\n",
    "        topic_descs.append(tag.text.strip())\n",
    "    return topic_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_urls(doc):\n",
    "    topic_link_tags=doc.find_all(\"a\",{\"class\":\"no-underline flex-1 d-flex flex-column\"})\n",
    "    topic_urls=[]\n",
    "    base_url = \"http://github.com\"\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base_url+tag['href'])\n",
    "    return topic_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Putting together all in a single function--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def scrape_topics():\n",
    "    topics_url =\"https://github.com/topics\"\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code!=200:\n",
    "        raise Exception('Failed to load page {}'.format(topics_url))\n",
    "    doc = BeautifulSoup(response.text,'html.parser')\n",
    "    topics_dict = {\n",
    "    'title':get_topic_titles(doc),\n",
    "    'description':get_topic_descs(doc),\n",
    "    'url':get_topic_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now getting top 30 repositories from topic page--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    # Download the page\n",
    "    response = requests.get(topic_url)\n",
    "    # Check successful response\n",
    "    if response.status_code!=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "    # Parse using Beautiful Soup\n",
    "    topic_doc = BeautifulSoup(response.text,'html.parser')\n",
    "    return topic_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The stars were in the form 60.7k, it needed a conversion therefore the below function-\n",
    "def parse_star_count(stars_str):\n",
    "    stars_str=stars_str.strip()\n",
    "    if stars_str[-1]=='k':\n",
    "        return int(float(stars_str[:-1])*1000)\n",
    "    return int(stars_str)\n",
    "\n",
    "# We create a fuction get_repo_info to fetch all the information we need about a repo \n",
    "def get_repo_info(h3_tag,star_tags):\n",
    "    a_tags = h3_tag.find_all('a')\n",
    "    username = a_tags[0].text.strip()\n",
    "    repo_name = a_tags[1].text.strip()\n",
    "    repo_url = \"http://github.com\" + a_tags[1]['href']\n",
    "    stars = parse_star_count(star_tags.text.strip())\n",
    "    return username,repo_name,stars,repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_repo(topic_doc):\n",
    "    # Get the h3 tags containing repo title,repo url and username\n",
    "    repo_tags = topic_doc.findAll('h3',{\"class\":\"f3 color-fg-muted text-normal lh-condensed\"})\n",
    "    # Get star tags \n",
    "    star_tags=topic_doc.find_all('span',{\"class\":\"Counter js-social-count\"})\n",
    "    # Get repo info\n",
    "    topic_repos_dict = {\n",
    "        'username':[],\n",
    "        'repo_name':[],\n",
    "        'stars':[],\n",
    "        'repo_url':[]\n",
    "    }\n",
    "\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i],star_tags[i])\n",
    "        topic_repos_dict['username'].append(repo_info[0])\n",
    "        topic_repos_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repos_dict['stars'].append(repo_info[2])\n",
    "        topic_repos_dict['repo_url'].append(repo_info[3])\n",
    "    return pd.DataFrame(topic_repos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting information fetched into a CSV file--\n",
    "import os\n",
    "\n",
    "def scrape_topic(topic_url,topic_name):\n",
    "    fname =  topic_name+'.csv'\n",
    "    if os.path.exists(fname):\n",
    "        print('The file {} already exists. Skipping... '.format(fname))\n",
    "        return\n",
    "    topic_df = get_topic_repo(get_topic_page(topic_url))\n",
    "    topic_df.to_csv(fname,index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together--\n",
    "\n",
    "- We have a funciton to get the list of topics.\n",
    "- We have a function to create a CSV file for scraped repos from a topics page.\n",
    "- Let's create a function to put them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print('Scrapping list of topics')\n",
    "    topics_df = scrape_topics()\n",
    "    for index,row in topics_df.iterrows():\n",
    "        print('Scraping top repositories for \"{}\"'.format(row['title']))\n",
    "        scrape_topic(row['url'],row['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it to scrape the top repos for the all the topics on the first page of https://github.com/topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapping list of topics\n",
      "Scraping top repositories for \"3D\"\n",
      "Scraping top repositories for \"Ajax\"\n",
      "Scraping top repositories for \"Algorithm\"\n",
      "Scraping top repositories for \"Amp\"\n",
      "Scraping top repositories for \"Android\"\n",
      "Scraping top repositories for \"Angular\"\n",
      "Scraping top repositories for \"Ansible\"\n",
      "Scraping top repositories for \"API\"\n",
      "Scraping top repositories for \"Arduino\"\n",
      "Scraping top repositories for \"ASP.NET\"\n",
      "Scraping top repositories for \"Atom\"\n",
      "Scraping top repositories for \"Awesome Lists\"\n",
      "Scraping top repositories for \"Amazon Web Services\"\n",
      "Scraping top repositories for \"Azure\"\n",
      "Scraping top repositories for \"Babel\"\n",
      "Scraping top repositories for \"Bash\"\n",
      "Scraping top repositories for \"Bitcoin\"\n",
      "Scraping top repositories for \"Bootstrap\"\n",
      "Scraping top repositories for \"Bot\"\n",
      "Scraping top repositories for \"C\"\n",
      "Scraping top repositories for \"Chrome\"\n",
      "Scraping top repositories for \"Chrome extension\"\n",
      "Scraping top repositories for \"Command line interface\"\n",
      "Scraping top repositories for \"Clojure\"\n",
      "Scraping top repositories for \"Code quality\"\n",
      "Scraping top repositories for \"Code review\"\n",
      "Scraping top repositories for \"Compiler\"\n",
      "Scraping top repositories for \"Continuous integration\"\n",
      "Scraping top repositories for \"COVID-19\"\n",
      "Scraping top repositories for \"C++\"\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary--\n",
    "- I managed to successfully fetch data from https://github.com/topics and store it in a CSV file format.\n",
    "## Future Scope--\n",
    "- I plan to use the data fetched here for Tree-based data perturbation and modify it to reduce squared deviation based on an attribute though none of the attribute here is private and need hiding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
